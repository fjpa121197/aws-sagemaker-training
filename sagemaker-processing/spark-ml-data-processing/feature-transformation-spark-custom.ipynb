{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Transformation with Amazon SageMaker and Spark\n",
    "\n",
    "Sometimes, when dealing with big data, standard data preprocessing techniques fall short in terms of performance because of the big data factor. In this notebook, we will show how to use Spark, a distributed processing framework, that can provide performance benefits when dealing with big data. We will use it with Amazon SageMaker Processing to run our preprocessing workload, but leveraging the Spark library to perform the data preparation tasks.\n",
    "\n",
    "**Note:** We need to build the Spark container by ourselves, that is why the directory container is needed, and we will also need docker installed to build an image and push it to ECR."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import general modules\n",
    "import boto3\n",
    "import sagemaker\n",
    "from time import gmtime, strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name francisco-learning to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role ARN successfully extracted\n"
     ]
    }
   ],
   "source": [
    "# Set global variables and initialize sessions to be used along notebook\n",
    "\n",
    "region = \"eu-west-2\" # Replace with your region\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "default_bucket = sagemaker_session.default_bucket() # Replace if you have another bucket in mind\n",
    "prefix_bucket = \"sagemaker-training-preprocessing\" # Use it in case you want to put your artifacts inside another directory\n",
    "\n",
    "# Get execution role arn to be used and perform operation in cloud\n",
    "try:\n",
    "    # get_execution_role() will only work within Sagemaker studio or notebook instance\n",
    "    role_arn = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    # Will need to get the role ARN by initializing a a new IAM session and get the role by their name\n",
    "    iam = boto3.client('iam')\n",
    "    role_arn = iam.get_role(RoleName='AmazonSageMaker-ExecutionRole-20230204T144648')['Role']['Arn']\n",
    "    print(\"Role ARN successfully extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "prefix = f\"{prefix_bucket}/spark-preprocess-demo/{timestamp_prefix}\"\n",
    "input_prefix = prefix + \"/input/raw/abalone\"\n",
    "input_preprocessed_prefix = prefix + \"/input/preprocessed/abalone\"\n",
    "model_prefix = prefix + \"/model\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 187.4 KiB/187.4 KiB (254.0 KiB/s) with 1 file(s) remaining\n",
      "download: s3://sagemaker-sample-files/datasets/tabular/uci_abalone/abalone.csv to .\\abalone.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-eu-west-2-247231311879/sagemaker-training-preprocessing/spark-preprocess-demo/2023-02-07-20-37-43/input/raw/abalone/abalone.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch the dataset from the SageMaker bucket and download it locally\n",
    "!aws s3 cp s3://sagemaker-sample-files/datasets/tabular/uci_abalone/abalone.csv .\n",
    "\n",
    "# Uploading the training data to S3 (default bucket)\n",
    "sagemaker_session.upload_data(path=\"abalone.csv\", bucket=default_bucket, key_prefix=input_prefix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build docker container\n",
    "\n",
    "We need to build a docker container with the Dockerfile inside the `container` directory. This will help us set up Spark and the master/worker nodes correctly.\n",
    "\n",
    "For this, do the following before running the next code cell.\n",
    "\n",
    "1. cd into container directory\n",
    "2. Run the following command `docker build -t sagemaker-spark-example .` Note: Make sure docker is installed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate variables names to be used for creating ECR registry, and push image.\n",
    "\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\") # Account number for AWS\n",
    "region = boto3.session.Session().region_name # Region name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For pushing the image to ECR, we will do the following:\n",
    "1. Create ECR repository, using the following command: \n",
    "\n",
    "    `aws ecr create-repository --repository-name sagemaker-spark-example`\n",
    "2. Log in into ECR to be able to push image, using the following command: \n",
    "\n",
    "    `aws ecr get-login-password --region eu-west-2 | docker login --username AWS --password-stdin {REPLACE_WITH_ACCOUNT_ID}.dkr.ecr.{REPLACE_WITH_REGION}.amazonaws.com`\n",
    "3. Build image to be pushed, using the following command: \n",
    "\n",
    "    `docker build -t sagemaker-spark-example .` \n",
    "    \n",
    "    Note: make sure to be inside `container directory`.\n",
    "4. Tag image with latest so it be pushed to ECR, using the following command: \n",
    "\n",
    "    `docker tag sagemaker-spark-example:latest {REPLACE_WITH_ACCOUNT_ID}.dkr.ecr.{REPLACE_WITH_REGION}.amazonaws.com/sagemaker-spark-example:latest`\n",
    "5. Push image to ECR repository, using the following command: \n",
    "\n",
    "    `docker push {REPLACE_WITH_ACCOUNT_ID}.dkr.ecr.{REPLACE_WITH_REGION}.amazonaws.com/sagemaker-spark-example:latest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_repository_uri = \"247231311879.dkr.ecr.eu-west-2.amazonaws.com/sagemaker-spark-example:latest\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Job\n",
    "\n",
    "Now that we have a container to be used, already in ECR, we can create the preprocessing job script that will use Spark (in this case, pyspark) to prepare the data.\n",
    "\n",
    "We will first create a preprocessing script, and then a preprocesing job definition using SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.py\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "def csv_line(data):\n",
    "    r = \",\".join(str(d) for d in data[1])\n",
    "    return str(data[0]) + \",\" + r\n",
    "\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.builder.appName(\"PySparkAbalone\").getOrCreate()\n",
    "\n",
    "    # Convert command line args into a map of args\n",
    "    args_iter = iter(sys.argv[1:])\n",
    "    args = dict(zip(args_iter, args_iter))\n",
    "\n",
    "    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\n",
    "        \"mapred.output.committer.class\", \"org.apache.hadoop.mapred.FileOutputCommitter\"\n",
    "    )\n",
    "\n",
    "    # Defining the schema corresponding to the input data. The input data does not contain the headers\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"sex\", StringType(), True),\n",
    "            StructField(\"length\", DoubleType(), True),\n",
    "            StructField(\"diameter\", DoubleType(), True),\n",
    "            StructField(\"height\", DoubleType(), True),\n",
    "            StructField(\"whole_weight\", DoubleType(), True),\n",
    "            StructField(\"shucked_weight\", DoubleType(), True),\n",
    "            StructField(\"viscera_weight\", DoubleType(), True),\n",
    "            StructField(\"shell_weight\", DoubleType(), True),\n",
    "            StructField(\"rings\", DoubleType(), True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Downloading the data from S3 into a Dataframe\n",
    "    total_df = spark.read.csv(\n",
    "        (\n",
    "            \"s3a://\"\n",
    "            + os.path.join(args[\"s3_input_bucket\"], args[\"s3_input_key_prefix\"], \"abalone.csv\")\n",
    "        ),\n",
    "        header=False,\n",
    "        schema=schema,\n",
    "    )\n",
    "\n",
    "    # StringIndexer on the sex column which has categorical value\n",
    "    sex_indexer = StringIndexer(inputCol=\"sex\", outputCol=\"indexed_sex\")\n",
    "\n",
    "    # one-hot-encoding is being performed on the string-indexed sex column (indexed_sex)\n",
    "    sex_encoder = OneHotEncoder(inputCol=\"indexed_sex\", outputCol=\"sex_vec\")\n",
    "\n",
    "    # vector-assembler will bring all the features to a 1D vector for us to save easily into CSV format\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[\n",
    "            \"sex_vec\",\n",
    "            \"length\",\n",
    "            \"diameter\",\n",
    "            \"height\",\n",
    "            \"whole_weight\",\n",
    "            \"shucked_weight\",\n",
    "            \"viscera_weight\",\n",
    "            \"shell_weight\",\n",
    "        ],\n",
    "        outputCol=\"features\",\n",
    "    )\n",
    "\n",
    "    # The pipeline comprises of the steps added above\n",
    "    pipeline = Pipeline(stages=[sex_indexer, sex_encoder, assembler])\n",
    "\n",
    "    # This step trains the feature transformers\n",
    "    model = pipeline.fit(total_df)\n",
    "\n",
    "    # This step transforms the dataset with information obtained from the previous fit\n",
    "    transformed_total_df = model.transform(total_df)\n",
    "\n",
    "    # Split the overall dataset into 80-20 training and validation\n",
    "    (train_df, validation_df) = transformed_total_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "    # Convert the train dataframe to RDD to save in CSV format and upload to S3\n",
    "    train_rdd = train_df.rdd.map(lambda x: (x.rings, x.features))\n",
    "    train_lines = train_rdd.map(csv_line)\n",
    "    train_lines.saveAsTextFile(\n",
    "        \"s3a://\" + os.path.join(args[\"s3_output_bucket\"], args[\"s3_output_key_prefix\"], \"train\")\n",
    "    )\n",
    "\n",
    "    # Convert the validation dataframe to RDD to save in CSV format and upload to S3\n",
    "    validation_rdd = validation_df.rdd.map(lambda x: (x.rings, x.features))\n",
    "    validation_lines = validation_rdd.map(csv_line)\n",
    "    validation_lines.saveAsTextFile(\n",
    "        \"s3a://\"\n",
    "        + os.path.join(args[\"s3_output_bucket\"], args[\"s3_output_key_prefix\"], \"validation\")\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name spark-preprocessor-2023-02-07-21-26-03-070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  spark-preprocessor-2023-02-07-21-26-03-070\n",
      "Inputs:  [{'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-eu-west-2-247231311879/spark-preprocessor-2023-02-07-21-26-03-070/input/code/preprocess.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  []\n",
      "........................................................................!"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ScriptProcessor, ProcessingInput\n",
    "\n",
    "# Define the preprocessing job, and use the container we just built by specifying an image_uri value.\n",
    "spark_processor = ScriptProcessor(\n",
    "    base_job_name=\"spark-preprocessor\",\n",
    "    image_uri=spark_repository_uri,\n",
    "    command=[\"/opt/program/submit\"],\n",
    "    role=role_arn,\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.t3.xlarge\", # Note: For free tier account, this instance will be not fall into that free tier.\n",
    "    max_runtime_in_seconds=1200,\n",
    "    env={\"mode\": \"python\"},\n",
    ")\n",
    "\n",
    "spark_processor.run(\n",
    "    code=\"preprocess.py\",\n",
    "    arguments=[\n",
    "        \"s3_input_bucket\",\n",
    "        default_bucket,\n",
    "        \"s3_input_key_prefix\",\n",
    "        input_prefix,\n",
    "        \"s3_output_bucket\",\n",
    "        default_bucket,\n",
    "        \"s3_output_key_prefix\",\n",
    "        input_preprocessed_prefix,\n",
    "    ],\n",
    "    logs=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets inspect the data from the preprocessing job. We will first download the data file (in this case, it is a text file) and then display the first 10 lines of that file.\n",
    "\n",
    "**Note:** The first column is the target variable, and the remaining columns are the features. There are no headers, because it is intended that the next step is to train a XGBoost model (which AWS algorithm requires no headers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --quiet s3://$default_bucket/$input_preprocessed_prefix/train/part-00000 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0,0.0,0.0,0.275,0.195,0.07,0.08,0.031,0.0215,0.025\n",
      "\n",
      "6.0,0.0,0.0,0.29,0.21,0.075,0.275,0.113,0.0675,0.035\n",
      "\n",
      "5.0,0.0,0.0,0.29,0.225,0.075,0.14,0.0515,0.0235,0.04\n",
      "\n",
      "7.0,0.0,0.0,0.305,0.225,0.07,0.1485,0.0585,0.0335,0.045\n",
      "\n",
      "7.0,0.0,0.0,0.305,0.23,0.08,0.156,0.0675,0.0345,0.048\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(r\"C:\\Users\\franc\\personal-coding-projects\\aws-sagemaker-training\\sagemaker-processing\\spark-ml-data-processing\\part-00000\") as f:\n",
    "    for _ in range(5): # first 5 lines\n",
    "        print(f.readline())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws-training-sm-pipelines",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07eb38fb7643f3f354709bacd544c125b1efa69b447b2c397e0b04cda751f292"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
